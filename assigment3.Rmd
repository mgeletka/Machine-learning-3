---
title: "Machine Learning, 2019/2020 - Assignment 3"
author: "Martin Geletka, Alessandra Crippa, Giulio Rago"
date: "June 03, 2020"
output: html_document
---


## Introduction
The purpose of this assignment is  to use generated artificial data to
study some properties of a few ANN architectures. The studied architectures are

* MLPs
* RNNs
* LSTMs

In this assignment, we will compare the behaviour of these mehtods in terms of accuracy and convergence and show the best results for each network


```{r}
library(keras, quietly=TRUE)
library(kerasR)
```


## Generating pseudo XML data
```{r}
gen_tag <- function(maxtaglen=5, all_letters=1) {
  taglen<-floor(runif(1)*maxtaglen+1)
  paste(letters[sample(1:(length(letters)*all_letters),taglen,replace=TRUE)], collapse='')
}
gen_line <- function(depth=5,maxtaglen,all_letters) {
  inner <- ""
  size <- runif(1)*7+3
  tag<-gen_tag(maxtaglen,all_letters)
  inner<-""
  while(runif(1)>0.4 && depth>0)
    if(inner=="")
      inner<-gen_line(depth=depth-1,maxtaglen,all_letters)
  else
    inner<-paste(inner,gen_line(depth=depth-1,maxtaglen,all_letters))
  endtag<-paste("<",tag,collapse='',sep='')
  if(inner=="")
    paste(tag,endtag)
  else
    paste(tag,inner,endtag)
}
# this function generates full sequences. You can truncate them using indexes.
gen_xml_data <- function(len,depth,maxtaglen,all_letters)
  sapply(1:len,function(i) gen_line(depth,maxtaglen,all_letters))
data <- gen_xml_data(len=2000, depth=2,maxtaglen=3,all_letters=0.3)
```
### Create simple dataset
```{r}
minLengtOfSequences = 10
data <- data[nchar(data) > minLengtOfSequences]
sequences <- sapply(data, function(x) {substr(x, start = 1, stop = minLengtOfSequences - 1)})
labels <- sapply(data, function(x) {substr(x, start = minLengtOfSequences - 1, stop = minLengtOfSequences - 1)})

df <- df <- data.frame(Sequence=sequences, Label=labels) 
summary(df)
```

### Preprocess input sequencies
```{r}
text <- data
max_features <- 28
tokenizer <- text_tokenizer(num_words = max_features, char_level=TRUE)
tokenizer %>% 
  fit_text_tokenizer(text)

text_seq <- texts_to_sequences(tokenizer, df$Sequence)
x_train <- matrix(unlist(text_seq), ncol = minLengtOfSequences-1, byrow = TRUE)
dim(x_train)

y_train <- as.matrix(as.double(as.factor(unlist(texts_to_sequences(tokenizer, df$Label)))))

#y_seq <- as.integer(texts_to_sequences(tokenizer, df$Label))
# y_seq <- y_seq - 1
#y_train <- to_categorical(as.matrix(y_seq),  num_classes=max(y_seq))

```



### MLP
```{r}

embedding_dims <- 50


model <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = 'relu', input_shape = c(minLengtOfSequences-1)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = max_features, activation = 'softmax')


model %>% compile(
  loss = 'sparse_categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

hist_mlp <- model %>%
  fit(
    x_train,
    y_train,
    batch_size = 16,
    epochs = 100,
    validation_split = 0.2
  )
```

```{r}
plot(hist_mlp)

```
### MLP-results
```{r}
hist_mlp
```

### RNN 
```{r}
modelRNN <- keras_model_sequential()

x_train_RNN <- array(x_train,  dim = c(dim(x_train)[1], minLengtOfSequences-1, 1))

# Topology
modelRNN %>%
    layer_simple_rnn(units = 64, input_shape = dim(x_train_RNN)[2:3])%>%
    layer_flatten() %>% 
    layer_dense(units = max_features, activation = 'softmax')

# loss, optimizer, metrics
modelRNN %>% compile(
  loss = 'sparse_categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

histRNN <- modelRNN %>%
  fit(
    x_train_RNN,
    y_train,
    batch_size = 16,
    epochs = 100,
    validation_split = 0.2
  )

plot(histRNN)
```

### RNN -results
```{r}
histRNN
```

### LSTM

```{r}
modelLSTM <- keras_model_sequential()

x_train_LSTM <- array(x_train,  dim = c(dim(x_train)[1], minLengtOfSequences-1, 1))

# Topology
modelLSTM %>%
    layer_lstm(units = 64, activation = 'relu', input_shape = dim(x_train), return_sequences=TRUE)%>%
    layer_dense(units = max_features, activation = 'softmax')

# loss, optimizer, metrics
modelLSTM %>% compile(
  loss = 'sparse_categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

histLSTM <- model %>%
  fit(
    x_train,
    y_train,
    batch_size = 16,
    epochs = 100,
    validation_split = 0.2
  )

plot(histLSTM)
```

### LSTM - results

```{r}
histLSTM
```

### Conslusion on basic data
Both MLP and RNN learn and converge pretty quickly, but  LSTM is learning much slower and it seems it not converge so quickly then other two models.
the Rnn is the model that learn more quickly, indeed it reaches an accuracy of 0.99 in more or less fifty epochs.
Also, the mls converge very quickly but it does not reach a satisfactory accuracy.
the lstm's accuracy grows in a similar to the linear way, so we can think that increasing the number of epochs we can reach a more satisfice accuracy and loss 


### Now try all model with more complicated data
In this model we increase the len of the sequence to 20000 instead of 
```{r}

big_data <- gen_xml_data(len=20000, depth=10,maxtaglen=15,all_letters=0.3)

```




```{r}
minLengtOfSequences = 10
big_data <- big_data[nchar(big_data) > minLengtOfSequences]
sequences <- sapply(big_data, function(x) {substr(x, start = 1, stop = minLengtOfSequences - 1)})
labels <- sapply(big_data, function(x) {substr(x, start = minLengtOfSequences - 1, stop = minLengtOfSequences - 1)})

df <- df <- data.frame(Sequence=sequences, Label=labels) 
summary(df)
```

```{r}
text <- big_data
max_features <- 28
tokenizer <- text_tokenizer(num_words = max_features, char_level=TRUE)
tokenizer %>% 
  fit_text_tokenizer(text)

text_seq <- texts_to_sequences(tokenizer, df$Sequence)
x_train_big <- matrix(unlist(text_seq), ncol = minLengtOfSequences-1, byrow = TRUE)
dim(x_train)

y_train_big <- as.matrix(as.double(as.factor(unlist(texts_to_sequences(tokenizer, df$Label)))))
```

### MLP
```{r}

embedding_dims <- 50


model <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = 'relu', input_shape = c(minLengtOfSequences-1)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = max_features, activation = 'softmax')


model %>% compile(
  loss = 'sparse_categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

hist_mlp_big <- model %>%
  fit(
    x_train_big,
    y_train_big,
    batch_size = 16,
    epochs = 100,
    validation_split = 0.2
  )

plot(hist_mlp_big)

```
```{r}
hist_mlp_big
```


### RNN - Big
```{r}
modelRNN <- keras_model_sequential()

x_train_RNN <- array(x_train_big,  dim = c(dim(x_train_big)[1], minLengtOfSequences-1, 1))

# Topology
modelRNN %>%
    layer_simple_rnn(units = 64, input_shape = dim(x_train_RNN)[2:3])%>%
    layer_flatten() %>% 
    layer_dense(units = max_features, activation = 'softmax')

# loss, optimizer, metrics
modelRNN %>% compile(
  loss = 'sparse_categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

histRNN_big <- modelRNN %>%
  fit(
    x_train_RNN,
    y_train_big,
    batch_size = 16,
    epochs = 100,
    validation_split = 0.2
  )

plot(histRNN_big)
```

```{r}
histRNN_big

```
### LSTM big
```{r}
modelLSTM <- keras_model_sequential()

x_train_LSTM <- array(x_train_big,  dim = c(dim(x_train)[1], minLengtOfSequences-1, 1))

# Topology
modelLSTM %>%
    layer_lstm(units = 64, activation = 'relu', input_shape = dim(x_train_big), return_sequences=TRUE)%>%
    layer_dense(units = max_features, activation = 'softmax')

# loss, optimizer, metrics
modelLSTM %>% compile(
  loss = 'sparse_categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

histLSTM_big <- model %>%
  fit(
    x_train_big,
    y_train_big,
    batch_size = 16,
    epochs = 100,
    validation_split = 0.2
  )

plot(histLSTM_big)
```


```{r}
histLSTM_big

```

With the increase of the dataset, our result change:
the Mls model is the one that remains more similar to the previous one, we can see how it converge,
the other two models assume a very strange because they converge super quickly to the best value possible.


```{r}

d_data <- gen_xml_data(len=20000, depth=15,maxtaglen=15,all_letters=0.4)

```


```{r}
minLengtOfSequences = 10
d_data <- d_data[nchar(d_data) > minLengtOfSequences]
sequences <- sapply(d_data, function(x) {substr(x, start = 1, stop = minLengtOfSequences - 1)})
labels <- sapply(d_data, function(x) {substr(x, start = minLengtOfSequences - 1, stop = minLengtOfSequences - 1)})

df <- df <- data.frame(Sequence=sequences, Label=labels) 
summary(df)
```


```{r}
text <- d_data
max_features <- 28
tokenizer <- text_tokenizer(num_words = max_features, char_level=TRUE)
tokenizer %>% 
  fit_text_tokenizer(text)

text_seq <- texts_to_sequences(tokenizer, df$Sequence)
x_train_d <- matrix(unlist(text_seq), ncol = minLengtOfSequences-1, byrow = TRUE)
dim(x_train)

y_train_d <- as.matrix(as.double(as.factor(unlist(texts_to_sequences(tokenizer, df$Label)))))
```

### MLP
```{r}

embedding_dims <- 50


model <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = 'relu', input_shape = c(minLengtOfSequences-1)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = max_features, activation = 'softmax')


model %>% compile(
  loss = 'sparse_categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

hist_mlp_d <- model %>%
  fit(
    x_train_d,
    y_train_d,
    batch_size = 16,
    epochs = 100,
    validation_split = 0.2
  )

plot(hist_mlp_d)

```


```{r}
hist_mlp_d
```



### RNN d
```{r}
modelRNN <- keras_model_sequential()

x_train_RNN <- array(x_train_d,  dim = c(dim(x_train_d)[1], minLengtOfSequences-1, 1))

# Topology
modelRNN %>%
    layer_simple_rnn(units = 64, input_shape = dim(x_train_RNN)[2:3])%>%
    layer_flatten() %>% 
    layer_dense(units = max_features, activation = 'softmax')

# loss, optimizer, metrics
modelRNN %>% compile(
  loss = 'sparse_categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

histRNN_d <- modelRNN %>%
  fit(
    x_train_RNN,
    y_train_d,
    batch_size = 16,
    epochs = 100,
    validation_split = 0.2
  )

plot(histRNN_d)
```


```{r}
histRNN_d
```

#### LMST

```{r}
modelLSTM <- keras_model_sequential()

x_train_LSTM <- array(x_train_d,  dim = c(dim(x_train)[1], minLengtOfSequences-1, 1))

# Topology
modelLSTM %>%
    layer_lstm(units = 64, activation = 'relu', input_shape = dim(x_train_d), return_sequences=TRUE)%>%
    layer_dense(units = max_features, activation = 'softmax')

# loss, optimizer, metrics
modelLSTM %>% compile(
  loss = 'sparse_categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

histLSTM_d <- model %>%
  fit(
    x_train_d,
    y_train_d,
    batch_size = 16,
    epochs = 100,
    validation_split = 0.2
  )

plot(histLSTM_d)
```


```{r}
histLSTM_d
```

the MLS  preserve its performance: it converges pretty fast and reaches a good level of accuracy.
The RNN keeps the trend of the last dataset: the convergence is obtained in a few numbers of epochs.
The LSTM change again its performance, in this case, the model converges but in a very slow way.